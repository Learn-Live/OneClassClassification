Experiment configuration
Dataset: mnist
Seed: 0
Fraction of Outliers: 0
First layer weight init by dictionary: True
PCA pre-processing? False
Norm used: l1
Global contrast normalization? True
ZCA Whitening? False
MNIST classes: 0 vs. all
MNIST representation dimensionality: 32
MNIST architecture used: 1
MNIST Network with bias terms? True


NeuralNet configuration
Loss: svdd
Pre-training? True
Solver: adam
Learning rate: 0.0001
Learning rate decay? False
Learning rate decay after epoch: 10
Learning rate drop? True
Learning rate drop in epoch: 50
Learning rate drop by factor: 10
Momentum: 0.9
Rho: None
Use Batch Normalization? True
Number of epochs: 2
Batch size: 200
Leaky ReLU: True

Regularization
Weight decay: True
C-parameter: 1000000.0
Dropout: False
Dropout architecture? False

Pre-Training Configuration:
Reconstruction loss: l2
Learning rate drop? False
Learning rate drop in epoch: 50
Learning rate drop by factor: 10
Weight decay: True
C-parameter: 1000.0

SVDD
Hard margin objective? True
Block coordinate descent used to solve R (and possibly c)? False
Is center c fixed? True
Solver for R: minimize_scalar
Optimization method if minimize_scalar: bounded
Objective on which R is optimized if LP: primal
Block coordinate descent applied from epoch: 10
(R,c) block update every k epoch with k=5
Reconstruction regularization: False
C_rec-parameter: 1000.0
Nu-parameter: 0.1
Mean initialization of c? True
Number of batches for mean initialization of c: all


Results

Train AUC: 0.0 %
Train accuracy: 95.61289978027344 %
Train time: 166.1245

Val AUC: 0.0 %
Val accuracy: 90.98179626464844 %

Test AUC: 97.3582 %
Test accuracy: 92.4800033569336 %
Test time: 0.0


Experiment configuration
Dataset: mnist
Seed: 0
Fraction of Outliers: 0
First layer weight init by dictionary: True
PCA pre-processing? False
Norm used: l1
Global contrast normalization? True
ZCA Whitening? False
MNIST classes: 0 vs. all
MNIST representation dimensionality: 32
MNIST architecture used: 1
MNIST Network with bias terms? True


NeuralNet configuration
Loss: svdd
Pre-training? True
Solver: adam
Learning rate: 0.0001
Learning rate decay? False
Learning rate decay after epoch: 10
Learning rate drop? True
Learning rate drop in epoch: 50
Learning rate drop by factor: 10
Momentum: 0.9
Rho: None
Use Batch Normalization? True
Number of epochs: 2
Batch size: 200
Leaky ReLU: True

Regularization
Weight decay: True
C-parameter: 1000000.0
Dropout: False
Dropout architecture? False

Pre-Training Configuration:
Reconstruction loss: l2
Learning rate drop? False
Learning rate drop in epoch: 50
Learning rate drop by factor: 10
Weight decay: True
C-parameter: 1000.0

SVDD
Hard margin objective? True
Block coordinate descent used to solve R (and possibly c)? False
Is center c fixed? True
Solver for R: minimize_scalar
Optimization method if minimize_scalar: bounded
Objective on which R is optimized if LP: primal
Block coordinate descent applied from epoch: 10
(R,c) block update every k epoch with k=5
Reconstruction regularization: False
C_rec-parameter: 1000.0
Nu-parameter: 0.1
Mean initialization of c? True
Number of batches for mean initialization of c: all


Results

Train AUC: 0.0 %
Train accuracy: 95.61289978027344 %
Train time: 160.3144

Val AUC: 0.0 %
Val accuracy: 90.98179626464844 %

Test AUC: 97.3582 %
Test accuracy: 92.4800033569336 %
Test time: 0.0


Experiment configuration
Dataset: mnist
Seed: 0
Fraction of Outliers: 0.0
First layer weight init by dictionary: False
PCA pre-processing? True
Norm used: l1
Global contrast normalization? False
ZCA Whitening? False
MNIST classes: 0 vs. all
MNIST representation dimensionality: 32
MNIST architecture used: 1
MNIST Network with bias terms? True


SVM configuration
Loss: OneClassSVM
Kernel: rbf
GridSearchCV for hyperparameter selection? True
Gamma: 0.0009765625
Nu-parameter: 0.8


Results

Train AUC: 0.0 %
Train accuracy: 19.9899 %
Train time: 3.4478

Val AUC: 98.7832 %
Val accuracy: 90.4 %

Test AUC: 94.2901 %
Test accuracy: 92.3222 %
Test time: 20.5124


